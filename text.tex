\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


% Chapter: Document classification
{\input{document_classification.tex}}


\chapter{Similar work}
    \section{LSI with class knowledge}
        \subsection{Supervised LSI}
            \cite{sun2004supervised} %supervised lsi, discriminative basis

        \subsection{Sprinkling}
            \cite{chakraborti2007supervised} % Sprinkling
        
        \subsection{Supervised weights} \label{sec:supervised:weights}
            \cite{wu2017balancing} % word weights, TODO read
            \cite{ji2013discriminative} % TF KLD
            \cite{deng2014study} % supervised weights
            \cite{lan2009supervised} % supervised weights for text categorization

    
    \section{SVD in neural networks}
        \subsection{SVD as structured layer} 
            \cite{ionescu2015training} % svd in neural networks

    
\chapter{Our approach}
    
    In this chapter we describe our motivations, the actual design of our approach and implementations details.
    
    \section{Goals and motivation}
    
    \* %TODO introduce minimal amount of hyperparameters,
    \* %TODO be able to get some insight and interpretability out of the model,

    In LSA, the SVD will inevitably throw away some information. 
    This is due the dimensionality reduction that it performs.
    The problem is, that this information may be vital to the classification task. 
    It is clear, that we need to introduce some form of supervision, that happens before the SVD.
    
    This is done to some extend by the supervised word weights described in section \ref{sec:supervised:weights}. 
    However, we consider this supervision rather weak, as it only computes simple statistics over words and labels.
    
    We would like to incorporate some form of direct supervision, that can directly affect which words are remembered by the SVD and which are thrown away.
    
    Because of the supervised weights, we know that supervision helps. 
    The only problem is, how to achieve it. 
    Here we take the inspiration from neural networks, and the insight that we can easily optimize parameters of rather complex system, as long as each part of this system is differentiable. 
    
    \section{Design}
    
    First recall how does the original LSA works in context of document classification.
    We start with a sentence $s_i$. We represent it as a sparse vector, a bag of words vector $d_i'$. 
    With some weighting scheme we transform it into a term vector $d_i$.
    Afterwards we compute the lower dimensional embedding of the document $v_i = d_i U$. 
    
    Finally we use these embeddings $v_i$ as features $x_i$ that are an input the the classifier (logistic regression, neural network, SVM).
    This classifier predicts some labels $\hat{y}$ that can be compared to
    the true labels $y$.
    
    Our contribution is to introduce another layer of reweighting. 
    Instead of using the term vector $d_i$, we introduce weights $w'$ and use reweighted vector as $d_i \circ w'$.
    Recall, that $\circ$ means Hadamard product (pointwise multiplication).
    This is equivalent to multiplying the vector $d_i$ by diagonal matrix with diagonal $w'$.
    
    This means, we do not perform the SVD decomposition on the matrix $M$, 
    but on the reweighted version $M \circ w'$. 
    Here we employ a few conventions from popular linear algebra library NumPy \cite{oliphant2006guide}, 
    where such operation is automatically broadcasted through the matrix.
    
    We initialize the $w'$ to be a vector of ones. 
    This effectively does not change the matrix at first.
    Now we are interested, in finding the best $w'$. 
    
    Recall, that for fixed decomposition $M\circ w'= U\Sigma V^T$ and some classifier $c$ (like a neural network),
    we compute $\hat{y} = g(M \circ w' U)$.
    We want to optimize $w'$ to minimize some error $E_y(\hat{y})$.
    The only question is, how to do it?
    
    The natural candidate is the gradient descent. 
    However to use it, we need to be able to compute the gradient of error with regards to the weights $\frac{\partial E_y(\hat{y})}{\partial w'}$
    
    \subsection{Gradient computation}
    
    To compute the gradient efficiently, we recall the back-propagation trick described in section \ref{sec:backprop}.
    We can compute the derivation $\frac{\partial E_y(\hat{y})}{\partial w'}$ thanks to the chain rule
    
    \begin{align}
    \frac{\partial E_y(\hat{y})}{\partial w'} = \frac{\partial E_y(\hat{y})}{\partial \hat{y}} \frac{\partial c(x)}{\partial x} \frac{\partial x}{\partial d}
    \end{align}
    
    To compute the derivation $\frac{\partial E_y(\hat{y})}{\partial w'} = \frac{\partial E_y(\hat{y})}{\partial \hat{y}} \frac{\partial c(x)}{\partial x} \frac{\partial x}{\partial d}$ we realize,
    that each part of this expression is easy to calculate.
    
    The first part of this expression is a function dependent on the chosen error function. 
    For L2 error we get 
    $\frac{\partial E_y(\hat{y})}{\partial \hat{y}} = \frac{\partial \frac{1}{2}(\hat{y}-y)^2}{\partial \hat{y}} = \hat{y}-y$
    
    The second part is dependent on the classifier that we use. 
    In case we use logistic regression $\hat{y} = \sigma(x \Theta + b)$, the expression becomes
    $\frac{\partial c(x)}{\partial x} = \hat{y} (1-\hat{y}) \Theta$
    Note, that it is easy to compute this expression even for more complicated classifiers. 
    Some libraries can even compute this automatically \cite{tensorflow2015-whitepaper}.
    However we still need that the classifier is differentiable, so we can not easily use SVM. 
    
    The third and last part is tied to the computation of LSA embedding.
    Because $x = d \circ w' U$, we can compute 
    $\frac{\partial x}{\partial d} = d U$ \* % TODO fix this math stuff
    
    Once we computed the gradient, we need to decide on the the optimization routine we employ.
    
    \subsection{Optimization routine}
    
    We want to perform the gradient step $w' = w' - \frac{\partial E_y(\hat{y})}{\partial w'}$. 
    However this changes the input to the SVD and we should recompute the decomposition as well. 
    However it is not clear, how often we want to recompute the SVD and how often (and how much) we want to retrain the classifier.
    We propose two main approaches inspired by neural network training and by 
    expectation minimization algorithm. \* % TODO cite something
    
    Our model can be broken down into three parts: weighting, decomposition and classification.
    We will denote the weighting part as $W$, the decomposition part as $S$ (SVD) and the classification part as $C$.
    Also as this parts are going to be updated through the time, we will denote them with superscripts $W^t$, referring to $W$ at time $t$.
    
    \* %todo, code EM algo (optimize w' while possible, then recompute LSA)
    
    \subsubsection{Batch gradient descent}
    In this setting, 
    
    
    \subsubsection{Stochastic gradient descent}
    
    We take the inspiration from stochastic gradient descent for neural networks.
    Instead of computing the full gradient, we just compute the gradient related to one example $\frac{\partial E_{y_i}(\hat{y_i})}{\partial w'}$
    Because the document vector $d_i$ is sparse, we will only update a few values from $w'$, 
    hence we can easily update part $W$.

    Because we updated $w'$, the entry in matrix $M$ corresponding to the document $d_i$ changed.
    We know, that SVD can be build incrementally and that we can add new documents to the matrix $M$ \cite{brand2006fast}.
    We just use the adding routine and add the new representation $d_i w'^{t+1}$ to the decomposition.
    Because we changed the reweighed document and the the decomposition matrices,
    the embedding of this document also changed to 
    $x_i^{t+1} = v_i^{t+1} = d_i w'^{t+1} U^{t+1}$
    
    We update $C$ in the similar manner.
    We just perform the stochastic gradient step for example $x_i^{t+1}$ and threat it as a new example in the dataset.
    
    \section{Implementation}
        
        In the implementation
        \cite{bird2009natural} % nlp in python
        \cite{oliphant2006guide} % numpy
        
        Our implementation is available on github \*% add link
        
        \subsection{Gensim lsa}
        \cite{rehurek_lrec}
        \* % TODO Intro about gensim
        \* % TODO cite the fast lsa in gensim
        \* % TODO, cite other important objects we use (dictionaries, sparse corpus), weights, word2 vec implementation
        
        \subsection{Scikit-learn}
        \cite{sklearn_api},
        \cite{scikit-learn}
        \* % TODO classifiers
        \* % TODO 
        

        \subsection{Tensorflow for clasifier?}
        \* % do i want to implement the clasifier in tf?
        \* % note about computing the gradient authomaticaly,
        \* % discuss not used because of fast svd is not there.
    
        \subsection{Other used tools}
        \cite{perez2007ipython} %ipython
        \* % experiments, evaluation and development
        \* % cite python 
        \cite{hunter2007matplotlib} %matplotlib
        \cite{mckinney2010data}
        
        
        

\chapter{Experiments and evaluation}

    Performance
    
    Result analysis
    
    Weight analysis
    
    Transfer learning?

    \section{Classification datasets}
        \subsection{Datasets}
            \cite{conneau2017supervised} % datasets, benchmarks

    \section{Interpretability}
        \cite{ribeiro2016should} % explainability


