\chapter{Experiments}

In this chapter we present our experimental results and compare them to baseline solutions.
We perform the evaluation on multiple classification tasks. 

\section{Classification datasets}
    
    Our datasets were acquired by following instructions in the SentEval\footnote{\url{https://github.com/facebookresearch/SentEval}} repository.
    SentEval is a library for evaluating the quality of sentence embeddings \cite{conneau2017supervised}. 
    However it focuses on evaluating unsupervised embeddings.
    We have not used any code from this library, because we were not able to modify its API in a maintainable way to fit our needs. 
    In the end we just used it as a guideline for datasets acquisition.
    
    \subsection{Overview} \label{sec:data:overview}
    We use a set of binary classification tasks that covers various types of sentence classification.
    Datasets corresponding to these tasks are:
    \begin{itemize}
        \item Movie review sentiment analysis MR.
        \item Product review dataset CR.
        \item Subjectivity/objectivity dataset SUBJ.
        \item Question-type dataset TREC.
        \item Opinion polarity dataset MPQA.
    \end{itemize}
    

\begin{table}[h]
\begin{center}

\begin{tabular}{l|rrrrr}
\toprule
{} &        CR &      MPQA &         MR &      SUBJ &      TREC \\\hline
\midrule
\#examples                                &   3775 &  10606 &   10662 &   10000 &   5952 \\\hline
\#unique words                            &   5674 &   6238 &   20325 &   22636 &   8968 \\\hline
\#words                                   &  75932 &  32779 &  230162 &  246015 &  58468 \\\hline
\specialcell{\#words with\\$1$ apperance} &   2714 &   3117 &   10160 &   11152 &   5338 \\\hline
\specialcell{avg sentence\\length}       &     20.11 &      3.09 &      21.59 &      24.60 &      9.82 \\\hline
\specialcell{max sentence\\length}       &    106 &     44 &      62 &     122 &     37 \\\hline
\specialcell{median sentence\\length}    &     18 &      2 &      21 &      23 &      9 \\\hline
bias                                     &      0.64 &      0.69 &       0.50 &       0.50 &       NaN \\
\bottomrule
\end{tabular}

\caption[Dataset characteristics]{Dataset characteristics}
\label{tab:datasets:stats}
\end{center}
\end{table}


    In table \ref{tab:datasets:stats} we present a few basic statistics for each dataset.
    Column \emph{bias} denotes percentage of the majority class in the dataset.
    It also presents a baseline accuracy which a naive algorithm could achieve by making constant predictions. 
    Bias for TREC dataset is not applicable for reasons discussed in section \ref{sec:TREC}.
    
    We discuss each of these datasets in more details and we provide some example sentences from these datasets.
    Note, that these examples  are displayed in very genuine. We see these examples as they really are and as they are presented to our algorithms.
    The actual preprocessing will be discussed later in this section.
    
    \subsubsection{Movie review, MR}
    
    With rise of internet forums and critique websites arose a problem of sentiment classification.
    
    This dataset contains movie reviews from site Rotten Tomatoes collected by Bo Pang and Lillian Lee \cite{pang2002thumbs}.
    Originally each comment is accompanied by categorical rating with values ``fresh'' (good movie) to ``rotten'' (bad movie). 
    These labels were transformed into this freely available dataset\footnote{\url{http://www.cs.cornell.edu/people/pabo/movie-review-data/}}.
    
    Example of positive review in this dataset is: \emph{the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .}

    Example of an negative review: \emph{simplistic , silly and tedious .}
    
    \subsubsection{Product review, CR}
    
    This dataset was introduced by Hu and Liu \cite{hu2004mining} as a part of a customer review summarization task. \* % write more, chceck this citation
    They created a pipeline with a summarization pipeline that consists of: mining product features that have been commented on by customers; identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative;  summarizing  the  results.  
    
    We are only interested in the sentiment prediction part.
    
    Example of a positive review:
    \emph{im a more happier person after discovering the i/p button ! .}

    Example of a negative review:
    \emph{weaknesses are minor : the feel and layout of the remote control are only so-so ; . it does n 't show the complete file names of mp3s with really long names ; . you must cycle through every zoom setting ( 2x , 3x , 4x , 1/2x , etc . ) before getting back to normal size [ sorry if i 'm just ignorant of a way to get back to 1x quickly ] .}

    \subsubsection{Opinion polarity MPQA}

    Another sentiment dataset collected by Wiebe \cite{wiebe2005annotating}.
    Its focus is to capture not only the overall tone of the document but also strength of the tone.
    However we will use only the binary positive-negative classification.
    Wiebe designed a fine grained annotation scheme and employed it on sentence corpus of articles from the world press.
    
    Example of a positive sentence:
    \emph{are also being encouraged}
    
    Example of a negative sentence:
    \emph{failing to support} 

    
    \subsubsection{Question-type, TREC}
    
    An important step in question answering and other dialog systems is to classify the  question to the anticipated type of the answer. 
    For example, the question of \emph{Who is a good boy?} should be classified into the type of animal (entity), because the question probably refers to some dog.  
    This information would narrow down the search space to identify the correct answer string \cite{huang2008question}. 
    
    This dataset contains questions and their labels in such classification.
    There are $6$ labels: 
    
    \begin{itemize}
        \item abbreviation (ABR): \emph{what is the full form of .com?}
        \item description (DESC): \emph{how did serfdom develop in and then leave russia?}
        \item entity (ENTY): \emph{what films featured the character popeye doyle?}
        \item human (HUM): \emph{what contemptible scoundrel stole the cork from my lunch?}
        \item location (LOC): \emph{what sprawling u.s. state boasts the most airports?}
        \item numeric (NUM): \emph{when was ozzy osbourne born?}
    \end{itemize}
    
    As can be seen, this dataset is not binary. 
    For our purposes, we create $6$ different binary datasets out of this one by employing \emph{one-vs-all} strategy (one-vs-the-rest) used while training prediction models.
    This strategy consists in fitting one classifier per class. 
    For each classifier, the class is fitted against all the other classes. 
    Hence new dataset TREC-ABR will contain the same samples, but labels will be binary, $1$ if the sentence was originally in class ABR and $0$ otherwise.
    
\begin{table}[h]
\begin{center}

\begin{tabular}{lrrrrrr}
\toprule
{} &  ABBR &  DESC &  ENTY &   HUM &   LOC &   NUM \\
\midrule
bias &  0.98 &  0.78 &  0.77 &  0.78 &  0.85 &  0.83 \\
\bottomrule
\end{tabular}

\caption[TREC subtasks characteristics]{TREC subtasks characteristics}
\label{tab:trec:stats}
\end{center}
\end{table}

    \subsubsection{Subjectivity/objectivity, SUBJ}
    
    There is number of sub-tasks that can help in context of sentiment prediction.
    One such task is to decide whether the text was subjective or objective.
        
    Pang and Lee were able to mine the Web and create a large, automatically labeled sentence corpus. 
    To gather subjective sentences they collected movie review snippets from website  \url{www.rottentomatoes.com}.
    To obtain objective data, they took sentences from plot summaries available from the Internet Movie Database (\url{www.imdb.com}) \cite{pang2004sentimental} \footnote{We personally think, that this technique is rather questionable at best.}.
    
    Example of a objective review:
    \emph{the movie begins in the past where a young boy named sam attempts to save celebi from a hunter .}

    Example of a subjective review:
    \emph{smart and alert , thirteen conversations about one thing is a small gem .}
    

    \subsubsection{Preprocessing} \label{sec:preprocessing}
    
    Examples provided for each dataset could be slightly strange at the first glance.
    We deliberately did only minimal processing in terms of word filtering and normalization.
    The datasets are already tokenized, we only lower-case the text.
    We do not filter any stop words (\texttt{the}) or non token characters (like question marks \texttt{?}). 
    Not filtering words that are generally filtered showed to be an important decision that allowed us to get an insight from data.
    
    LSA is commonly accompanied by heavy filtering and it is known, that stop words removal helps the performance.
    Our rational behind minimal filtering is, that we would like the weighting scheme to understand, which words are important.
    Hence we hope, that if the stop word is really not important, its weight will be reduced.
    
    \subsection{Metrics}
    
    Because we deal with binary classification tasks, our metric of choice is an accuracy.
    
    Accuracy of our $p$ predictions $\hat{y}$ given the true labels $y$ can be computed as
    $$acc_y(\hat{y}) = \frac{1}{p}\sum_{i=1}^py_i ==\hat{y}_i$$
    
    Note that this metric can have problems with unbalanced datasets. 
    We just need to remember, that what is a good accuracy dpeends on the dataset.
    For example accuracy $0.69$ on MPQA dataset is really bad, because this accuracy can be achieved by constant classifier.
    In later experiments we will refer to the row with biases in table \ref{tab:datasets:stats}.
    
    We will compute accuracy on tree subsets of the dataset. 
    $\mathrm{acc}_{train}$ on the train set, $\mathrm{acc}_{valid}$ on the validation set and $\mathrm{acc}_{test}$ on the test set.
    Note that $\mathrm{acc}_{train}$ is not wery interesting on its own, but if we compare it with $\mathrm{acc}_{valid}$ and $\mathrm{acc}_{test}$ we can get an insight about whether our model is overfitting or underfitting.



\section{Results}
    In this section we present accuracy achieved by our approach and by baseline approaches.
    To address the problems of measuring accuracy, we report relative improvements over a constant baseline.
    For absolute precisions refer to the appendix B. 

    Note, that because we have multiple testing datasets and multiple methods, we witness a small combinatorial explosion.

    \subsection{Baselines} \label{sec:baseline}

    There is a number of baselines that we consider.
    These are commonly used approaches such as pretrained neural embeddings, or very naive baselines such as constant classifier.
    Further we report accuracy of standard LSA.

    \subsubsection{Constant baselines}
    As discussed in section \label{sec:data:overview}, some of our datasets are biased. 
    They may contain substantially more examples belonging to one label that belonging to the other.
    Because of this, we can ``train'' a very simple constant classifier. 
    This classifier counts number of samples in each class in the training set and than always outputs the majority class.
    
    Performance of this classifier is almost identical to the bias of given dataset. 
    There are minor differencies because of the random data split. 
    
    \subsubsection{BOW baselines}    
    
    First commonly used baseline is a BOW representation with logistic regression as a classifier. 
    We test multiple different supervised and unsupervised word weighting schemes.
    
    It would be argued, that using term weights is not necessary when using logistic regression as the classifier.
    The argument is, that the term weighs could in theory be absorbed into the weights of the classifier. 
    This argument actually true if we only consider the TF part of the weighting scheme.
    
    However, this argument only holds for the optimal solution. 
    The problem is, that even though an equivalent solution exists, it may not be found by the learning algorithm. 
    In practice it is useful to add the weighting scheme. 
    This introduces some form of a bias, because we tell then the algorithm which words are probably more important.
    Because of that we may employ more strict regularization techniques without significant decrees in the performance.
    

\begin{table}[H]
\begin{center}

\begin{tabular}{llrrrr}
\toprule
{} &      &  CR &  MPQA &  MR &  SUBJ \\
scheme &  &            &              &            &              \\
\midrule
None & test &      0.143 &        0.159 &      0.270 &        0.419 \\
{} & train &      0.335 &        0.223 &      0.479 &        0.494 \\
tfchi2 & test &      0.111 &        0.125 &      0.176 &        0.349 \\
{} & train &      0.147 &        0.166 &      0.206 &        0.343 \\
tfgr & test &      0.116 &        0.126 &      0.176 &        0.349 \\
{} & train &      0.150 &        0.167 &      0.211 &        0.346 \\
tfidf & test &      0.124 &        0.160 &      0.258 &        0.416 \\
{} & train &      0.361 &        0.291 &      0.500 &        0.500 \\
tfig & test &      0.116 &        0.126 &      0.176 &        0.349 \\
{} & train &      0.150 &        0.167 &      0.211 &        0.346 \\
tfor & test &      0.135 &        0.151 &      0.273 &        0.404 \\
{} & train &      0.253 &        0.198 &      0.407 &        0.436 \\
tfrf & test &      0.106 &        0.147 &      0.239 &        0.386 \\
{} & train &      0.194 &        0.179 &      0.322 &        0.418 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for BOW baseline]{Precision improvements for BOW baseline}
\label{tab:}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}

\begin{tabular}{llrrrrrr}
\toprule
{} &&  ABBR &  DESC &  ENTY &  HUM &  LOC &  NUM \\
scheme &  & & & &&&\\
\midrule
None & test & 0.011 & 0.148 & 0.108 &0.138 &0.117 &0.129 \\
{} & train & 0.010 & 0.202 & 0.209 &0.202 &0.142 &0.158 \\
tfchi2 & test & 0.009 & 0.083 & 0.006 &0.102 &0.104 &0.097 \\
{} & train & 0.009 & 0.098 & 0.015 &0.113 &0.103 &0.092 \\
tfgr & test & 0.006 & 0.081 & 0.006 &0.104 &0.104 &0.106 \\
{} & train & 0.007 & 0.097 & 0.014 &0.112 &0.103 &0.103 \\
tfidf & test & 0.011 & 0.148 & 0.108 &0.134 &0.127 &0.139 \\
{} & train & 0.016 & 0.218 & 0.226 &0.216 &0.154 &0.170 \\
tfig & test & 0.006 & 0.081 & 0.006 &0.104 &0.104 &0.106 \\
{} & train & 0.007 & 0.097 & 0.014 &0.112 &0.103 &0.103 \\
tfor & test & 0.006 & 0.066 & 0.063 &0.139 &0.107 &0.129 \\
{} & train & 0.006 & 0.170 & 0.166 &0.183 &0.123 &0.140 \\
tfrf & test & 0.006 & 0.061 & 0.060 &0.124 &0.102 &0.118 \\
{} & train & 0.007 & 0.136 & 0.135 &0.158 &0.104 &0.125 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for BOW baseline on TREC datasets]{Precision improvements for BOW baseline on TREC datasets}
\label{tab:}
\end{center}
\end{table}

    \* % describe results


    \subsubsection{LSA baselines}
    The most important baseline that we need to consider is LSA.
    In this baseline we construct the term matrix $M$, reweigh it with weighting scheme, train the LSA embedding and train the classifier.
    We try multiple weighting schemes and multiple classifiers.
    
    Refer to tables \ref{tab:lsa:resuts:300} and \ref{tab:lsa:resuts:400} in appendix B for resuts for dimensions $300$ and $400$.
    
\begin{table}[H]
\begin{center}

\begin{tabular}{llrrrr}
\toprule
{} &      &  CR &  MPQA &  MR &  SUBJ \\
scheme &  &            &              &            &              \\
\midrule
None & test &      0.093 &        0.095 &      0.198 &        0.360 \\
{} & train &      0.162 &        0.103 &      0.216 &        0.390 \\
tfchi2 & test &      0.093 &        0.095 &      0.198 &        0.360 \\
{} & train &      0.162 &        0.103 &      0.215 &        0.390 \\
tfgr & test &      0.093 &        0.094 &      0.198 &        0.359 \\
{} & train &      0.162 &        0.103 &      0.215 &        0.390 \\
tfidf & test &      0.093 &        0.095 &      0.197 &        0.360 \\
{} & train &      0.162 &        0.103 &      0.215 &        0.389 \\
tfig & test &      0.093 &        0.094 &      0.197 &        0.360 \\
{} & train &      0.162 &        0.103 &      0.216 &        0.390 \\
tfor & test &      0.093 &        0.094 &      0.197 &        0.360 \\
{} & train &      0.162 &        0.103 &      0.215 &        0.389 \\
tfrf & test &      0.093 &        0.094 &      0.197 &        0.359 \\
{} & train &      0.162 &        0.103 &      0.215 &        0.389 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for LSA baseline with 200 dimensions]{Precision improvements for LSA baseline with 200 dimensions}
\label{tab:}
\end{center}
\end{table}

    %comapre to word vectos, compare to logistic regression, discuss overfitting.
    
    \* % results    

    \subsubsection{Neural embeddings}
    In this baseline we train word2vec neural embeddings on the dataset. 
    We use implementation from gensim \inlinecode{Python}{gensim.models.Word2Vec}.
    Te train embeddings with dimensions $200$, $300$ and $400$. 
    Than we compute embedding for each word and sum them into an sentence embeddings.
    Than we train SVC or logistic regression and present the precision.
    
    Note, that we train embeddings for each dataset separately.

\begin{table}[H]
\begin{center}

\begin{tabular}{lllrrrr}
\toprule
 & &&CR &MPQA &MR &SUBJ \\
\midrule
logistic & 200 & test & 0.03 &0.0 & 0.10 & 0.31 \\
 & & train & 0.03 &0.0 & 0.11 & 0.31 \\
 & 300 & test & 0.01 &0.0 & 0.09 & 0.31 \\
 & & train & 0.02 &0.0 & 0.11 & 0.31 \\
 & 400 & test & 0.01 &0.0 & 0.09 & 0.32 \\
 & & train & 0.02 &0.0 & 0.11 & 0.31 \\
svc & 200 & test &-0.00 &0.0 & 0.05 & 0.17 \\
 & & train & 0.00 &0.0 & 0.08 & 0.19 \\
 & 300 & test &-0.00 &0.0 & 0.05 & 0.16 \\
 & & train & 0.00 &0.0 & 0.08 & 0.18 \\
 & 400 & test &-0.00 &0.0 & 0.05 & 0.14 \\
 & & train & 0.00 &0.0 & 0.08 & 0.16 \\
\bottomrule
\end{tabular}


\caption[Precision improvements for trained word vectors]{Precision improvements for trained word vectors}
\label{tab:res:trainedwordvec}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}

\begin{tabular}{lllrrrrrr}
\toprule
 & &&ABBR &DESC &ENTY &HUM &LOC &NUM \\
\midrule
logistic & 200 & test &-0.0 &0.00 &-0.0 & 0.03 &0.0 & 0.06 \\
 & & train &-0.0 &0.01 &-0.0 & 0.02 &0.0 & 0.05 \\
 & 300 & test &-0.0 &0.01 &-0.0 & 0.01 & -0.0 & 0.05 \\
 & & train &-0.0 &0.00 &-0.0 & 0.01 & -0.0 & 0.04 \\
 & 400 & test &-0.0 &0.00 &-0.0 & 0.00 & -0.0 & 0.04 \\
 & & train &-0.0 &0.00 &-0.0 & 0.01 & -0.0 & 0.03 \\
svc & 200 & test &-0.0 &0.00 &-0.0 &-0.00 & -0.0 & 0.00 \\
 & & train &-0.0 & -0.00 & 0.0 & 0.00 & -0.0 & 0.00 \\
 & 300 & test &-0.0 &0.00 &-0.0 &-0.00 & -0.0 & 0.00 \\
 & & train &-0.0 & -0.00 & 0.0 & 0.00 & -0.0 & 0.00 \\
 & 400 & test &-0.0 &0.00 &-0.0 &-0.00 & -0.0 & 0.00 \\
 & & train &-0.0 & -0.00 & 0.0 & 0.00 & -0.0 & 0.00 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for trained word vectors on TREC dataset]{Precision improvements for trained word vectors on TREC dataset}
\label{tab:res:trainedwordvec:trec}
\end{center}
\end{table}


    Tables \ref{tab:res:trainedwordvec} and \ref{tab:res:trainedwordvec:trec} contain our results.
    We see zero improvement of SVM classifier for TREC datasets for all embedding sizes.
    TREC dataset seems to be overall very challenging dataset.
    Interesting observation is, that precision on this dataset decreases as we increase the embedding dimensions.
    The highest precision increase ($0.05$) on TREC dataset is achieved by logistic regression and embeddings size $200$. 
    
    We see the biggest improvement on the SUBJ dataset ($0.31$), with no notable dependency on embedding dimension size.  
    
    We also conclude that we have not observed any overfitting as train and test precisions are very similar.
    

    \subsubsection{Pre-trained neural embeddings}    
    
    In this experiment we use pretrained word embeddings. 
    We use python library spacy.
    It provides easy interface for obtaining pretrained word embeddings.
    
    We use \inlinecode{Python}{spacy.load('en_vectors_web_lg')}.
    This command returns a pretrained model that we can query with words. 
    Embedding obtained in this way have $300$ dimensions and were trained on a large web corpus (large collection of websites). 
    Note, that because these vectors were trained on much bigger datasets, it is not a very fair comparison to other methods presented here.

\begin{table}[h]
\begin{center}

\begin{tabular}{llrrrr}
\toprule
 &&CR &MPQA &MR &SUBJ \\
\midrule
logistic & test & 0.18 & 0.20 & 0.29 & 0.43 \\
 & train & 0.19 & 0.20 & 0.29 & 0.42 \\
svc & test &-0.00 & 0.17 & 0.24 & 0.40 \\
 & train & 0.00 & 0.18 & 0.24 & 0.40 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for pretrained word vectors]{Precision improvements for pretrained word vectors}
\label{tab:res:pretrainedwordvec}
\end{center}
\end{table}



\begin{table}[H]
\begin{center}

\begin{tabular}{llrrrrrr}
\toprule
 &&ABBR &DESC &ENTY &HUM &LOC &NUM \\
\midrule
logistic & test &0.01 &0.08 &0.07 & 0.14 & 0.10 & 0.07 \\
 & train &0.01 &0.11 &0.09 & 0.14 & 0.10 & 0.10 \\
svc & test & -0.00 &0.02 & -0.00 & 0.09 & 0.00 & 0.00 \\
 & train & -0.00 &0.03 &0.00 & 0.09 & 0.01 & 0.00 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for pretrained word vectors on TREC dataset]{Precision improvements for pretrained word vectors on TREC dataset}
\label{tab:res:pretrainedwordvec:trec}
\end{center}
\end{table}

    
    We see, that these embeddings perform much better than the trained ones. 
    We even see a significant improvements on the TREC dataset (absolute precision over $0.90$).
    Interesting observation is, that logistic regression performs much better than SVM.
    Note, that we have not spend much time on finetuning hyperparameters of these classifiers. 

    
    \subsection{Our approach}
    
    In our approach we evaluate a number of hyperparameters. Most notably the classifier, used term weights and type of learning (batch vs stochastic).
    
    With batch, we evaluate how many w updates we perform before we recompute the SVD and retrain the classifier.
    
    \* %write
    we test number of weights
    initializations
    \texttt{tfidf} \texttt{tfchi2} \texttt{tfig} \texttt{tfgr} \texttt{tfor} \texttt{tfrf} \texttt{None}

    multiple learning rates and multiple embedding sizes on all datasets
    We compare our results agains lsas baseline (lsa with given weights, but without training).
    


\section{Interpretability}
    Most machine learning algorithm are black box approaches \cite{ribeiro2016should}. % explainability
    It is hard, to justify their predictions or to extract some insight from them. 
    
    Because our approach just re-weights words, we artue that it is explainable and we can extract important insight about our data from it.

