\chapter{Experiments}

In this chapter we present our experimental results and compare them to baseline solutions.
We perform the evaluation on multiple classification tasks. 

\section{Classification datasets}
    
    Our datasets were acquired by following instructions in the SentEval (\url{https://github.com/facebookresearch/SentEval}) repository.
    SentEval is a library for evaluating the quality of sentence embeddings \cite{conneau2017supervised}. 
    However it focuses on evaluating unsupervised embeddings.
    We have not used any code from this library, because we were not able to modify its API in a maintainable way to fit our needs. 
    In the end we just used it as a guideline for datasets acquisition.
    
    \subsection{Overview} \label{sec:data:overview}
    We use a set of binary classification tasks that covers various types of sentence classification.
    Datasets corresponding to these tasks are:
    \begin{itemize}
        \item Movie review sentiment analysis MR.
        \item Product review dataset CR.
        \item Subjectivity/objectivity dataset SUBJ.
        \item Question-type dataset TREC.
        \item Opinion polarity dataset MPQA.
    \end{itemize}
    

\begin{table}[h]
\begin{center}

\begin{tabular}{l|rrrrr}
\toprule
{} &        CR &      MPQA &         MR &      SUBJ &      TREC \\\hline
\midrule
\#examples                                &   3775 &  10606 &   10662 &   10000 &   5952 \\\hline
\#unique words                            &   5674 &   6238 &   20325 &   22636 &   8968 \\\hline
\#words                                   &  75932 &  32779 &  230162 &  246015 &  58468 \\\hline
\specialcell{\#words with\\$1$ apperance} &   2714 &   3117 &   10160 &   11152 &   5338 \\\hline
\specialcell{avg sentence\\length}       &     20.11 &      3.09 &      21.59 &      24.60 &      9.82 \\\hline
\specialcell{max sentence\\length}       &    106 &     44 &      62 &     122 &     37 \\\hline
\specialcell{median sentence\\length}    &     18 &      2 &      21 &      23 &      9 \\\hline
bias                                     &      0.64 &      0.69 &       0.50 &       0.50 &       NaN \\
\bottomrule
\end{tabular}

\caption[Dataset characteristics]{Dataset characteristics}
\label{tab:datasets:stats}
\end{center}
\end{table}


    In table \ref{tab:datasets:stats} we present a few basic statistics for each dataset.
    Column \emph{bias} denotes percentage of the majority class in the dataset.
    It also presents a baseline accuracy which a naive algorithm could achieve by making constant predictions. 
    Bias for TREC dataset is not applicable for reasons discussed in section \ref{sec:TREC}.
    
    We discuss each of these datasets in more details and we provide some example sentences from these datasets.
    Note, that these examples  are displayed in very genuine. We see these examples as they really are and as they are presented to our algorithms.
    The actual preprocessing will be discussed later in this section.
    
    \subsubsection{Movie review, MR}
    
    With rise of internet forums and critique websites arose a problem of sentiment classification.
    
    This dataset contains movie reviews from site Rotten Tomatoes collected by Bo Pang and Lillian Lee \cite{pang2002thumbs}.
    Originally each comment is accompanied by categorical rating with values ``fresh'' (good movie) to ``rotten'' (bad movie). 
    These labels were transformed into this freely available dataset\footnote{\url{http://www.cs.cornell.edu/people/pabo/movie-review-data/}}.
    
    Example of positive review in this dataset is: \emph{the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .}

    Example of an negative review: \emph{simplistic , silly and tedious .}
    
    \subsubsection{Product review, CR}
    
    This dataset was introduced by Hu and Liu \cite{hu2004mining} as a part of a customer review summarization task. \* % write more, chceck this citation
    They created a pipeline with a summarization pipeline that consists of: mining product features that have been commented on by customers; identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative;  summarizing  the  results.  
    
    We are only interested in the sentiment prediction part.
    
    Example of a positive review:
    \emph{im a more happier person after discovering the i/p button ! .}

    Example of a negative review:
    \emph{weaknesses are minor : the feel and layout of the remote control are only so-so ; . it does n 't show the complete file names of mp3s with really long names ; . you must cycle through every zoom setting ( 2x , 3x , 4x , 1/2x , etc . ) before getting back to normal size [ sorry if i 'm just ignorant of a way to get back to 1x quickly ] .}

    \subsubsection{Opinion polarity MPQA}

    Another sentiment dataset collected by Wiebe \cite{wiebe2005annotating}.
    Its focus is to capture not only the overall tone of the document but also strength of the tone.
    However we will use only the binary positive-negative classification.
    Wiebe designed a fine grained annotation scheme and employed it on sentence corpus of articles from the world press.
    
    Example of a positive sentence:
    \emph{are also being encouraged}
    
    Example of a negative sentence:
    \emph{failing to support} 

    
    \subsubsection{Question-type, TREC}
    
    An important step in question answering and other dialog systems is to classify the  question to the anticipated type of the answer. 
    For example, the question of \emph{Who is a good boy?} should be classified into the type of animal (entity), because the question probably refers to some dog.  
    This information would narrow down the search space to identify the correct answer string \cite{huang2008question}. 
    
    This dataset contains questions and their labels in such classification.
    There are $6$ labels: 
    
    \begin{itemize}
        \item abbreviation (ABR): \emph{what is the full form of .com?}
        \item description (DESC): \emph{how did serfdom develop in and then leave russia?}
        \item entity (ENTY): \emph{what films featured the character popeye doyle?}
        \item human (HUM): \emph{what contemptible scoundrel stole the cork from my lunch?}
        \item location (LOC): \emph{what sprawling u.s. state boasts the most airports?}
        \item numeric (NUM): \emph{when was ozzy osbourne born?}
    \end{itemize}
    
    As can be seen, this dataset is not binary. 
    For our purposes, we create $6$ different binary datasets out of this one by employing \emph{one-vs-all} strategy (one-vs-the-rest) used while training prediction models.
    This strategy consists in fitting one classifier per class. 
    For each classifier, the class is fitted against all the other classes. 
    Hence new dataset TREC-ABR will contain the same samples, but labels will be binary, $1$ if the sentence was originally in class ABR and $0$ otherwise.
    
\begin{table}[h]
\begin{center}

\begin{tabular}{lrrrrrr}
\toprule
{} &  ABBR &  DESC &  ENTY &   HUM &   LOC &   NUM \\
\midrule
bias &  0.98 &  0.78 &  0.77 &  0.78 &  0.85 &  0.83 \\
\bottomrule
\end{tabular}

\caption[TREC subtasks characteristics]{TREC subtasks characteristics}
\label{tab:trec:stats}
\end{center}
\end{table}

    \subsubsection{Subjectivity/objectivity, SUBJ}
    
    There is number of sub-tasks that can help in context of sentiment prediction.
    One such task is to decide whether the text was subjective or objective.
        
    Pang and Lee were able to mine the Web and create a large, automatically labeled sentence corpus. 
    To gather subjective sentences they collected movie review snippets from website  \url{www.rottentomatoes.com}.
    To obtain objective data, they took sentences from plot summaries available from the Internet Movie Database (\url{www.imdb.com}) \cite{pang2004sentimental} \footnote{We personally think, that this technique is rather questionable at best.}.
    
    Example of a objective review:
    \emph{the movie begins in the past where a young boy named sam attempts to save celebi from a hunter .}

    Example of a subjective review:
    \emph{smart and alert , thirteen conversations about one thing is a small gem .}
    

    \subsubsection{Preprocessing} \label{sec:preprocessing}
    
    Examples provided for each dataset could be slightly strange at the first glance.
    We deliberately did only minimal processing in terms of word filtering and normalization.
    The datasets are already tokenized, we only lower-case the text.
    We do not filter any stop words (\texttt{the}) or non token characters (like question marks \texttt{?}). 
    Not filtering words that are generally filtered showed to be an important decision that allowed us to get an insight from data.
    
    LSA is commonly accompanied by heavy filtering and it is known, that stop words removal helps the performance.
    Our rational behind minimal filtering is, that we would like the weighting scheme to understand, which words are important.
    Hence we hope, that if the stop word is really not important, its weight will be reduced.
    
    \subsection{Metrics}
    
    Because we deal with binary classification tasks, our metric of choice is an accuracy.
    
    Accuracy of our $p$ predictions $\hat{y}$ given the true labels $y$ can be computed as
    $$acc_y(\hat{y}) = \frac{1}{p}\sum_{i=1}^py_i ==\hat{y}_i$$
    
    Note that this metric can have problems with unbalanced datasets. 
    We just need to remember, that what is a good accuracy dpeends on the dataset.
    For example accuracy $0.69$ on MPQA dataset is really bad, because this accuracy can be achieved by constant classifier.
    In later experiments we will refer to the row with biases in table \ref{tab:datasets:stats}.
    
    We will compute accuracy on tree subsets of the dataset. 
    $\mathrm{acc}_{train}$ on the train set, $\mathrm{acc}_{valid}$ on the validation set and $\mathrm{acc}_{test}$ on the test set.
    Note that $\mathrm{acc}_{train}$ is not wery interesting on its own, but if we compare it with $\mathrm{acc}_{valid}$ and $\mathrm{acc}_{test}$ we can get an insight about whether our model is overfitting or underfitting.



\section{Results}
    In this section we present accuracy achieved by our approach and by baseline approaches.
    To address the problems of measuring accuracy, we report relative improvements over a constant baseline.
    For absolute precisions refer to the appendix B. 

    \subsection{Baselines} \label{sec:baseline}

    There is a number of baselines that we consider.
    These are commonly used approaches such as pretrained neural embeddings, or very naive baselines such as constant classifier.
    Further we report accuracy of standard LSA.

    \subsubsection{Constant baselines}
    As discussed in section \label{sec:data:overview}, some of our datasets are biased. 
    They may contain substantially more examples belonging to one label that belonging to the other.
    Because of this, we can ``train'' a very simple constant classifier. 
    This classifier counts number of samples in each class in the training set and than always outputs the majority class.
    
    Performance of this classifier is almost identical to the bies of given dataset. 
    There are ninor differencies. 
    
    \subsubsection{BOW baselines}    
    BOW + logistic regression
    We try multiple weighting schemes.
    Could argue, that the term weight in weighting schemes is redundant, because it can be absorbed into weight at the first layer.
    However, they are not redundant, they introduce a form of bias to the network (we can use regularization more freely).


    \subsubsection{Neural embeddings}    
    We train the word2vec on the dataset and then train the classifier
    Te train embeddings with dimensions $200$, $300$ and $400$ on each dataset. 
    Than we compute embedding words each word and sum them into an sentence embedding.
    Than we train SVC or logistic regression and present the precision.

\begin{table}[H]
\begin{center}

\begin{tabular}{lllrrrr}
\toprule
 & &&CR &MPQA &MR &SUBJ \\
\midrule
logistic & 200 & test & 0.03 &0.0 & 0.10 & 0.31 \\
 & & train & 0.03 &0.0 & 0.11 & 0.31 \\
 & 300 & test & 0.01 &0.0 & 0.09 & 0.31 \\
 & & train & 0.02 &0.0 & 0.11 & 0.31 \\
 & 400 & test & 0.01 &0.0 & 0.09 & 0.32 \\
 & & train & 0.02 &0.0 & 0.11 & 0.31 \\
svc & 200 & test &-0.00 &0.0 & 0.05 & 0.17 \\
 & & train & 0.00 &0.0 & 0.08 & 0.19 \\
 & 300 & test &-0.00 &0.0 & 0.05 & 0.16 \\
 & & train & 0.00 &0.0 & 0.08 & 0.18 \\
 & 400 & test &-0.00 &0.0 & 0.05 & 0.14 \\
 & & train & 0.00 &0.0 & 0.08 & 0.16 \\
\bottomrule
\end{tabular}


\caption[Precision improvements for trained word vectors]{Precision improvements for trained word vectors}
\label{tab:res:trainedwordvec}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}

\begin{tabular}{lllrrrrrr}
\toprule
 & &&ABBR &DESC &ENTY &HUM &LOC &NUM \\
\midrule
logistic & 200 & test &-0.0 &0.00 &-0.0 & 0.03 &0.0 & 0.06 \\
 & & train &-0.0 &0.01 &-0.0 & 0.02 &0.0 & 0.05 \\
 & 300 & test &-0.0 &0.01 &-0.0 & 0.01 & -0.0 & 0.05 \\
 & & train &-0.0 &0.00 &-0.0 & 0.01 & -0.0 & 0.04 \\
 & 400 & test &-0.0 &0.00 &-0.0 & 0.00 & -0.0 & 0.04 \\
 & & train &-0.0 &0.00 &-0.0 & 0.01 & -0.0 & 0.03 \\
svc & 200 & test &-0.0 &0.00 &-0.0 &-0.00 & -0.0 & 0.00 \\
 & & train &-0.0 & -0.00 & 0.0 & 0.00 & -0.0 & 0.00 \\
 & 300 & test &-0.0 &0.00 &-0.0 &-0.00 & -0.0 & 0.00 \\
 & & train &-0.0 & -0.00 & 0.0 & 0.00 & -0.0 & 0.00 \\
 & 400 & test &-0.0 &0.00 &-0.0 &-0.00 & -0.0 & 0.00 \\
 & & train &-0.0 & -0.00 & 0.0 & 0.00 & -0.0 & 0.00 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for trained word vectors on TREC dataset]{Precision improvements for trained word vectors on TREC dataset}
\label{tab:res:trainedwordvec:trec}
\end{center}
\end{table}


We see, that logistic regression performs better than support vector machine. Biggest improvements are on SUBJ dataset.


    \subsubsection{Pre-trained neural embeddings}    
    We get pretrained word2vec embeddings, sum them and train the classifier (logistic vs SVM)
    we use spacy $300$ dimensional word vectors pretrained on wikipedia. 
    This is not very fair comparison.
    

\begin{table}[H]
\begin{center}

\begin{tabular}{llrrrr}
\toprule
 &&CR &MPQA &MR &SUBJ \\
\midrule
logistic & test & 0.18 & 0.20 & 0.29 & 0.43 \\
 & train & 0.19 & 0.20 & 0.29 & 0.42 \\
svc & test &-0.00 & 0.17 & 0.24 & 0.40 \\
 & train & 0.00 & 0.18 & 0.24 & 0.40 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for pretrained word vectors]{Precision improvements for pretrained word vectors}
\label{tab:}
\end{center}
\end{table}



\begin{table}[H]
\begin{center}

\begin{tabular}{llrrrrrr}
\toprule
 &&ABBR &DESC &ENTY &HUM &LOC &NUM \\
\midrule
logistic & test &0.01 &0.08 &0.07 & 0.14 & 0.10 & 0.07 \\
 & train &0.01 &0.11 &0.09 & 0.14 & 0.10 & 0.10 \\
svc & test & -0.00 &0.02 & -0.00 & 0.09 & 0.00 & 0.00 \\
 & train & -0.00 &0.03 &0.00 & 0.09 & 0.01 & 0.00 \\
\bottomrule
\end{tabular}

\caption[Precision improvements for pretrained word vectors on TREC dataset]{Precision improvements for pretrained word vectors on TREC dataset}
\label{tab:}
\end{center}
\end{table}

    
    \subsection{Our approach}
    
    In our approach we evaluate a number of hyperparameters. Most notably the classifier, used term weights and type of learning (batch vs stochastic).
    
    With batch, we evaluate how many w updates we perform before we recompute the SVD and retrain the classifier.
    

    


\section{Interpretability}
    \cite{ribeiro2016should} % explainability


