\chapter{Similar work}
In literature, we identified $3$ ways how to address shortcomings of LSA for document classification. In this chapter we discuss these approaches in more details.

We can modify the term matrix $M$ so that weight of each word represents how important is this word for the actual classification task and use \emph{supervised term weights}. 
This is a contrast to the classical approach, where the weight of the word represents, how important is it for the document reconstruction.

Besides that, we can introduce some form of supervision directly into the matrix decomposition process.
Sun et al. \cite{sun2004supervised} propose sLSI (supervised latent semantic indexing) and they employ the supervision while choosing the most relevant dimensions of the embedding.
Chakraborti et al. \cite{chakraborti2006sprinkling} propose to add class dependent terms (supervised terms) into the term matrix and proceed with standard factorization.

Finally, we can fully employ advances made in artificial neural networks and use LSA just as a module into these networks.

In this chapter, we introduce each of these approaches in more details.


\section{LSA with class knowledge} \label{sec:superwisedlsa}
    \subsection{Supervised LSI}
        Recall (from section \ref{sec:lsa}) that we consider LSI and LSA to refer to the same method. 
    
        Sun et al. \cite{sun2004supervised} %supervised lsi, discriminative basis
        also recognized that LSI is not optimal for document categorization tasks because it finds the most representative features and not the most discriminative ones. 
        To address this problem, they incorporate the available classification of documents into $c$ categories and propose supervised latent semantic indexing (SLSI).
        
        To introduce their approach we will recapitulate the classical LSI first. 
        LSI selects the eigenvector of the term matrix $M$ with the biggest eigenvalues.
        The effect of this basis vector is then removed from the original document vector and we iterate this process. 
        
        SLSI algorithm employs the class information to guide the selection of the basis vectors. 
        For each category they perform standard SVD on the matrix $M$.
        Then they collect eigenvectors with the highest eigenvalues into a collection of candidate vectors.
        
        For each of these candidate vectors $g_s$ they project all documents onto this vector.
        They compute centroids of each class $v_s^p = \frac{1}{|C_p|}\sum_{d\in C_p} g_s d^T$ (average position of documents from given class). 
        Where $C_p$ denotes all documents belonging to class $p$.
        Note that because they project onto one vector, this centroid is a scalar. 
        Then they compute distances between such centroids for each pair of classes $f_s^{pq} = |v_s^p - v_s^q|$.
        
        Finally they select the most discriminative vector as $b=arg_{g_i \in G} \{\max_{p q} f_i^{p q} \}$ (vector that separates some pair of classes the best) and proceed to select the next one.
        
        They report that this algorithm generates better representations than standard LSI and can achieve higher dimensionality reduction without reducing the classification accuracy.
        
    \subsection{Sprinkling}
        Chakraborti et al. propose another way how to incorporate class information into LSI \cite{chakraborti2006sprinkling}\cite{chakraborti2007supervised}.
        They propose to add the label directly to the term matrix $M$ resulting into a ``sprinkled'' matrix $M'$.

\begin{figure}[h]
\centering
    \begin{equation*}
\begin{matrix} 
\begin{matrix} 
M  \\
\textbf{d}_i^T  \\
\downarrow \\
\begin{bmatrix}
x_{1,1}& \dots&  x_{1,N} \\
\vdots &  \ddots & \vdots \\
x_{j,1}& \dots & x_{j,N} \\
\vdots & \ddots & \vdots \\
x_{|D|,1} &\dots&  x_{|D|,N} \\
\end{bmatrix}
\end{matrix}
&
&
\begin{matrix} 
M'  \\
\textbf{d}_i^T  \\
\downarrow \\
\begin{bmatrix}
x_{1,1}& \dots&  x_{1,N} \\
\vdots &  \ddots & \vdots \\
x_{j,1}& \dots & x_{j,N} \\
\vdots & \ddots & \vdots \\
x_{|D|,1} &\dots&  x_{|D|,N} \\
\hline 
y_{1,1} &\dots&  y_{N,1} \\
\vdots & \ddots & \vdots \\
y_{1,k} &\dots&  y_{N,k} \\

\end{bmatrix}
\end{matrix}
\end{matrix}
\end{equation*}

\caption{Effect of sprinkling on term matrix}
\end{figure}

    Label related terms $y_{i,k}$, the sprinkled terms, have value $1$ if the document $d_i$ belongs to the class $k$ and $0$ otherwise.
    These terms are added as normal words into each document.
    Note, that Chakraborti et al. use transposed form of $M$ (documents are rows of $M$).
    We decided to use this variant to be consistent with the rest of the thesis (section \ref{sec:design}.
    
    After obtaining the matrix $M'$, they perform standard SVD. 
    Then they ``unspringle'' the matrix by removing word vectors associated with sprinkled terms.
    
    This approach reports $1\%$ improvement over LSI without sprinkling.
    
    \subsubsection{Why does it work?}
    Work of Kontostathis et al. \cite{kontostathis2006framework} reveals close correspondence between LSI and higher order associations between terms. 
    We say that word $w_1$ has a first order association with another word $w_2$, if they co-occur in at least one document. 
    We say that words $w_1$ and $w_2$ share a second order association if there is at least one term $w_3$ that co-occurs with $w_1$ and $w_2$ in distinct documents. 
    Kontostathis et al. provide experimental evidence to show that LSI boosts similarity between terms sharing higher order associations. 
    Sprinkled terms create such higher order associations between terms related to  the same class/category.
        
\section{Supervised weights} \label{sec:supervised:weights}
    
    Term weighting is a strategy that assigns weights to terms to improve the performance of sentiment analysis or other text mining tasks.
    If the reweighted vectors are used for some form of classification,
    we can enhance the weighting scheme by the class information.
    
    There is a number of ways, how to design such supervised term weights.
    In this thesis, we will build on top of some of those schemes,
    because we believe that supervised weights are a good approach to combat the shortcomings of LSA for document classification.
    
    In this chapter we introduce some of the interesting results in this field and some of the weighting schemes we will use.


    \subsection{Supervised weights for sentiment classification}
    Deng et al. \cite{deng2014study} % supervised weights
    proposed a number of superwised weighting schemes.
    \* % TODO write more about superwised weights
    IDT:
    ITS:
    
        'tfidf'
'tfchi2'
'tfig'
'tfgr'
'tfor'
'tfrf'
'None'

    
    
    A number of supervised weighting schemes was also studies by Wu et al. \cite{wu2017balancing} % word weights, TODO read
    and Lan et al. \cite{lan2009supervised}. % supervised weights for text categorization


    \subsection{TF-KLD}
        Ji and Eisenstein \cite{ji2013discriminative} % TF KLD
        studied the performance of LSA for semantic relatedness task.
        
        In semantic relatedness task we have two documents $d_i^1, d_i^2$ and we want to determine, whether they are semantically related or not. 
        Example of semantic relatedness task can be to determine, if two questions asked on some forum are the same (and should be deduplicated) or not \footnote{\url{https://www.kaggle.com/c/quora-question-pairs}}.
        
        For this task, Ji and Eisenstein proposed to compute the LSA embeddings of both texts $v_i^1$ and $v_i^2$ and train classifier on features based on these embeddings.
        In the paper they used $[v_1 + v_2, \| v_1- v_2 \|]$.
        
        They recognized, that LSA with term matrix $M$, containing term frequencies, performs poorly. 
        They designed novel weighting scheme called TF-KLD (term frequency Kullback-Leibler divergence). 
        This weighting scheme has classical term frequency part and second novel KLD part.
        
        KLD part describes, how important is given word for the task.
        They compute probabilities $p_j = P(s_{ij}^1| s_{ij}^2, y_i=1)$ and $q_j = P(s_{ij}^1| s_{ij}^2, y_i=0)$.
        In other words, what is the probability that word $w_j$ is in the first sentence if it is the second one and the sentences are (not) related.
        
        KLD for word $j$ is $KL(p_j || q_j) = \sum_x p_j(x) \log \frac{p_j(x)}{q_j(x)}$, where $KL$ denotes the Kullback-Leibler divergence.
        This can be seen as an amount of information that presence of the word tells us about similarity of this pair of sentences. 
    
        This approach with TF-KLD weights can perform even better, than recurrent neural networks \cite{conneau2017supervised}.
        This result shows, that with proper supervised weights, we can even outperform complicated neural network approaches.
        

\section{SVD in neural networks} \label{sec:nn:svd}
    Ionescu et al. \cite{ionescu2015training} % svd in neural networks
    proposed a very interesting method of how to use good properties of SVD inside a neural network.
    They researched the field of computer vision, that is currently dominated by convolutional neural networks (CNNs).
    
    They argue, that CNNs specialize at local computations while on the other hand SVD can perform global computations.
    
    They propose to use matrix factorization as a module inside the neural network.
    They also proposed a \emph{matrix backpropagation} methodology to train these networks.
    They compare their approach against widely used convolutional architectures VGG and Alexnet and report that addition of such structured layer significantly improved the segmentation accuracy.
    
    They compute the exact update that is necessary to perform on matrices $U$, $\sigma$, $V$ that describe the SVD.
    However, this computation is only possible, because they use the full SVD (not only the top $k$ dimensions).
    Because of this we cannot directly use their approach.
    However, their results show, that it is possible to ``flow the gradient through SVD''.
    Hence gradient descent can be used to update parameters that are used before the SVD.
    
    