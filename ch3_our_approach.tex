\chapter{eLSA}

In this chapter we introduce our novel approach eLSA: error boosted LSA.
We describe our motivations, the design of eLSA and some implementation details.

\section{Goals and motivation}

    The root of LSA's problems is the dimensionality reduction.
    It will inevitably throw away some information. 
    However, this information may be vital to the classification task. 
    It is clear, that we need to address this problem and introduce some form of supervision, that precedes the SVD.
    
    To some extend, this can be done by the supervised word weights described in section~\ref{sec:supervised:weights}.
    However, we consider this supervision to be rather weak, as it only computes simple statistics over the words and labels.
    
    In the end, we want to train some classifier $C$ on top of the document embeddings.
    We would like to find a way, how to use the knowledge about the classifiers error and incorporate it before the SVD.
    We achieve it by training our own word weighting scheme. 
    
    Our goals are to:
    \begin{itemize}
        \item Propose a novel approach for learning task-specific word weights.
        \item Experimentally evaluate the performance of the new approach.
        \item Compare the new approach with other (commonly used) weighting schemes. 
        \item Extract new insights about the dataset.
    \end{itemize}
    
    As a side goal, we would like to introduce as few new hyperparameters as possible,
    because they may cause a ``false improvement'' of our approach over the baseline.
    We also would like our approach to be at least slightly interpretable.
    It would be nice, if it could produce some insight to the dataset.
    
    Because of the supervised weights, we know that supervision helps. 
    The only problem is, how to achieve it. 
    Here we take the inspiration from neural networks, and the insight that we can easily optimize parameters of rather complex system, as long as each part of this system is differentiable. 

\section{Design} \label{sec:design}

    First, recall how does the LSA works in the context of document classification.
    We start with a sentence $s_i$ and we represent it as a bag of words vector $d_i'$. 
    Then we transform it with a weighting scheme into a term vector $d_i$.
    Afterwards we compute the lower dimensional embedding of the document $v_i = d_i U$. 
    Finally we use this embedding $v_i$ as an input $x_i$ to the classifier $C$ (logistic regression, neural network or SVM).
    This classifier predicts label $\hat{y_i}$ that can be compared to
    the true labels $y_i$.
    
    Our contribution is to introduce another layer of reweighting. 
    Instead of using the term vector $d_i$, we introduce weights $w'$ and use reweighted vector $d_i \circ w'$\footnote{Recall, that $\circ$ means Hadamard product (pointwise multiplication).}.
    This is equivalent to multiplying the vector $d_i$ by diagonal matrix with diagonal $w'$.
    Then we perform the SVD decomposition on the reweighted matrix $M \circ w'$. 
    Here, we employ a few conventions from popular linear algebra library NumPy~\cite{oliphant2006guide}, 
    where such operation is automatically broadcasted through the matrix.
    
    We initialize the $w'$ to be a vector of ones (this effectively does not change the matrix at first).
    For fixed decomposition $M\circ w'= U\Sigma V^T$ and some classifier $C$ (like a neural network),
    we compute $\hat{y} = C(M \circ w' U)$.
    Then we want to optimize $w'$ to minimize the error function $E_y(\hat{y})$.
    The only question is, how to do it.
    
    The natural choice is to employ the gradient descent algorithm. 
    However to use it, we need to be able to compute the gradient of error with regards to the weights $\frac{\partial E_y(\hat{y})}{\partial w'}$.
    
    \subsection{Gradient computation}
    
    To compute the gradient efficiently, we use the backpropagation trick mentioned in section~\ref{sec:backprop}.
    We can compute the derivation $\frac{\partial E_y(\hat{y})}{\partial w'}$ thanks to the chain rule
    
    \begin{equation} \label{eq:derivation}
    \frac{\partial E_y(\hat{y})}{\partial w'} = \frac{\partial E_y(\hat{y})}{\partial \hat{y}} \frac{\partial c(x)}{\partial x} \frac{\partial x}{\partial w'}
    \end{equation}
    
    The first part of expression~\ref{eq:derivation} is dependent on the chosen error function. 
    For L2 error we get 
    \begin{equation}
    \frac{\partial E_y(\hat{y})}{\partial \hat{y}} = \frac{\partial \frac{1}{2}(\hat{y}-y)^2}{\partial \hat{y}} = \hat{y}-y
    \end{equation}
    The second part is dependent on the classifier that we use. 
    In case we use logistic regression $\hat{y} = \sigma(x \Theta + b)$, the expression becomes
    \begin{equation}
    \frac{\partial c(x)}{\partial x} = \hat{y} (1-\hat{y}) \Theta
    \end{equation}
    Note that it is possible to compute this expression even for more complicated classifiers. 
    Some libraries can even compute this automatically~\cite{tensorflow2015-whitepaper}.
    However we need the classifier to be differentiable so we cannot easily use SVM as the classifier. 
    
    The third and last part is tied to the computation of the LSA embedding.
    Because $x = d \circ w' U$, we can compute 
    \begin{equation}
    \frac{\partial x_j}{\partial 'w_i} = d_i U_{i,j}
    \end{equation}
    
    This can be rewritten in more concise notation as 
    \begin{equation}
        \frac{\partial x}{\partial 'w} = U \circ d^T
    \end{equation}
    We again employ the convention that the $\circ$ operation is broadcasted across all columns of the matrix.

    Once we know how to compute the gradient, we need to decide on what optimization routine we will employ.

\section{Optimization routines}
    
    Inspired by gradient descent algorithm, we perform the gradient step 
    \begin{equation}
        w' = w' - \beta \frac{\partial E_y(\hat{y})}{\partial w'}
    \end{equation}
    The parameter $\beta$ determines the size of the gradient step that we will perform.
    We will call this parameter $w'$-learning rate. 
    Note that the classifier $C$ may have its of learning rate $\alpha$, and these two learning rates are completely independent.
    
    The problem is, that equation~\ref{eq:derivation} holds only for fixed classifier $C$ and for fixed decomposition $S$. 
    We ignore the fact, that $S$ is actually a function of reweighted matrix and hence it is a function of $w'$.
    It is probably possible to compute the exact gradient (refer to section~\ref{sec:nn:svd}), but we will not do that.
    
    It is not clear, how often we want to recompute the SVD and how often (and how much) we want to retrain the classifier.
    We propose two main approaches inspired by neural network training and by 
    expectation minimization algorithm.
    
    We use the notation introduced in section~\ref{sec:lsa}.
    However, our parts $W$, $S$ and $C$ are going to be updated through the time. We denote them with superscripts, e.g.~$W^t$, referring to weighting part $W$ at time $t$.
    
    \subsection{Batch gradient descent}
    
    We need to find weights $w'$ for the part $W$, 
    such that the decomposition $S$ captures the most information and hence the classifier $C$ has access to all the important features of the document.
    However, the decomposition $S$ is dependent on the reweighted matrix produced by $W$. 
    Because of this, after each update of $w'$ we need to fully recompute the decomposition $S$ and retrain the classifier $C$.
    The first optimization routine is described in the algorithm~\ref{algo:batch}.
    
    \medskip
    
    \begin{algorithm}[H]
        \KwData{$M$}
        \KwResult{trained $W$, $S$, $C$ }
        $w'^0 = 1$, $t=1$\;
        \While{performance is improving on validation dataset, $t++$}{
            recompute $S^t$ from $M \circ w'^{t-1}$\;
            compute embeddings $v^t$ from $M \circ w'^{t-1}$ with $S^t$\;
            fully train $C^t$ on $v^t$ and labels $y$\;
            update $W^t$: $w'^t = w'^{t-1} - \beta \frac{\partial E(C(S(M w'^{t-1}))}{\partial w'^{t-1}} $\;
        }
        \caption{stochastic training of $w'$} \label{algo:batch}
    \end{algorithm}
    
    We took inspiration from expectation maximization algorithm.  
    Note that it is not essential in any way for the reader to know this algorithm for further reading.
    
    We name the algorithm~\ref{algo:batch} eLSA, because it uses error of the classifier to produce boosted (improved) weight $w'$.
    
    This algorithm needs to do a lot of computations for one update of the weights $w'$.
    To address this, we can increase the number of gradient steps performed on $w'$ in each step of the while loop.
    We recall the same problem in neural networks was solved by stochastic gradient descent.
    We explore such solution as well.
    
    \subsection{Stochastic gradient descent}
    
    We take the inspiration from stochastic gradient descent step from neural networks.
    Instead of computing the full gradient, we just compute the gradient related to one example 
    $$\frac{\partial E_{y_i}(\hat{y_i})}{\partial w'}$$
    Because the document vector $d_i$ is sparse, we will only update a few values from $w'$, hence we can easily update part $W$.
    
    Because we updated $w'$, the entry in matrix $M$ corresponding to the document $d_i$ has changed.
    We know, that SVD can be build incrementally and that we can add new documents to the matrix $M$~\cite{brand2006fast}.
    We just use the adding routine and add the new representation $d_i \circ w'^{t}$ to the decomposition.
    Because we changed the reweighted document and the decomposition matrices,
    the embedding of this document should also change to 
    \begin{equation}
    x_i^{t} = v_i^{t} = d_i w'^{t} U^{t}
    \end{equation}
    
    We update $C$ in the similar manner.
    We perform the stochastic gradient step for example $x_i^{t+1}$ and threat it as a new example in the dataset.
    Pseudocode is available in algorithm~\ref{algo:stochastic}.
    
    \begin{algorithm}[H]
        \KwData{$M$}
        \KwResult{trained $W$, $S$, $C$ }
        $w'^0 = 1$, $t=1$\;
        compute $S^0$ on $M \circ w'^{0}$\;
        compute embeddings $v^0$ from $M \circ w'^{0}$ with $S^0$\;
        fully train $C^0$ on $v^0$ and labels $y$\;
        
        \While{performance is improving on validation dataset}{
            \For{document $d_i \in M$, $i++$, $t++$}{
                add $d_i$ to $S^{t-1}$ and get $S^t$\;
                compute embedding $v_i^{t}$ from $d_i \circ w'^{t-1}$ with $S^t$\;
                perform gradient step on $C^{t-1}$ with $v_i^t$ to get $C^t$\;
                update $W^t$, $w'^t = w'^{t-1} - \beta \frac{\partial E(C(S(d_i \circ w'^{t-1}))}{\partial w'^{t-1}} $\;    
            }
        }
        \caption{Batch training of $w'$} \label{algo:stochastic}
    \end{algorithm}
    
    We done a few preliminary experiments and this optimization routine showed to be very unstable.
    It also introduced a lot of new decisions that would have had been made and a lot of new hyperparameters.
    We do not explore it further.
    
    
    \subsection{eLSA explanation}
    We need to address a few things that are no apparent from the description provided above.
    We do it here to not clutter the notation and so that the previous sections are a bit cleaner. 
    
    \subsubsection{Train test split} \label{sec:train:test:split}
    
    Motivated by good practice from neural network training,
    we split our dataset into $3$ parts: train, validation, and test.
    Train set is $80\%$ of our samples, validation and test are both $10\%$.
    We make sure that all those sets have percentage of positive and negative labels that is similar to the whole dataset.
    
    We use the train dataset for gradient evaluations and overall training. 
    We use the validation dataset to keep track of our performance on unseen data.
    Based on this performance, we decide if the model is still improving, or if it started to overfit.
    Finally we report the performance on the test set, that was never seen by the model.
    
    Because our datasets are not very big, we stabilize our results by employing the crossvalidation technique.  
    However because we need to run a lot of experiments, we only use $3$ fold crossvalidation.
    
    \subsubsection{Stopping conditions} \label{sec:elsa:stopping}
    
    In both optimization routines we iteratively update parameters $w'$ and we do so while our performance on validation set is increasing.
    This needs more clarification.
    
    In each time step $t$ we evaluate the loss $E_{valid}^t$ on the validation dataset.
    By convergence we mean, that the $E_{valid}$ stopped improving.
    In the final solution we track the mean of this errors in last few time steps.
    When this mean does not increase for long enough, we stop the training. 
    Formally we track 
    $ME_{10}^t = \frac{1}{10} \sum_{0\leq i < 10} E_{valid}^{t-i}$ (mean for last $10$ iterations) and
    $ME_{10,20}^t = \frac{1}{10} \sum_{10\leq i < 20} E_{valid}^{t-i}$ (mean for previous $10$ iterations).
    
    We stop the training in time $t$ if $ME_{10}^t > ME_{10,20}^t$, which means our validation error started to increase and we need to stop.
    
    \subsubsection{Weighting schemes}
    
    An important properly of eLSA algorithm is, that we do not need to start from basic co-occurrence matrix $M$. 
    We can apply some weighted scheme, such as TF-IDF on this matrix first and then run eLSA.
    What weighting scheme we starts from is actually one of the important hyperparameters.
    
    Afterwards, we can examine the found $w'$ and see, which words had high value $w'_i$.
    This means, they were underweighted by the weighting scheme.

    \subsubsection{Hyperparameters} \label{sec:hyperparams}
    
    eLSA on its own has only three hyperparameters and that is the $w'$-learning rate $\beta$ and the initial weighting scheme. 
    Second hyperparameter is the dimensionality of the embedding. 
    This hyperparameter is not specific to eLSA.
    However eLSA can be viewed as a tool for analyzing weighting schemes so it is actually no a hyperparameter.
    
    In fact the SVD decomposition (part $S$) and the classifier ($C$) may have a lot of hyperparameters on their own.
    Exploring the space of those hyperparameters would be extremely computationally demanding and could be subject to further research.
    Instead we leave this parameters to reasonable defaults suggested by implementations we use.
    We can do this, because we are not interested in pushing the state of the art in document classification problem, we just want to see a relative improvement over the LSA baseline.


\section{Implementation}
    
    In this section we go through some implementation details and decisions.
    The whole thesis, including experiment evaluations, is implemented in Python3.
    For code, please refer to appendix~\ref{appendix:code}

    \subsection{Libraries} 

    While implementing our approach, we made use of multiple tools and libraries that deserves to be mentioned.

    \subsubsection{Scikit-learn}
    
    Scikit-learn (or sklearn) is a free machine learning library for Python programming language~\cite{scikit-learn}.
    It features wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems.
    It is extensible and provides very nice and clean APIs~\cite{sklearn_api}.
    
    In this thesis, we used it for multiple purposes.
    We used base classes for weighting schemes \texttt{TfidfTransformer}, \texttt{BaseEstimator}, \texttt{TransformerMixin}.
    We also use this library to generate training and test splits. 
    
    Lastly, we used scikits implementation of \texttt{LogisticRegression}.
    However, in our approach we need to compute a derivative of our classifiers. 
    This implementation does not provide an API for this computation and we had to use
    its internal variables \texttt{coef\_} and \texttt{intercept\_} that corresponds to the weights and bias of the classifier.
    
    \subsubsection{Gensim}
    
    Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python~\cite{rehurek_lrec}. 
    It is an efficient and hassle-free software to realize unsupervised semantic modelling from plain text~\cite{bird2009natural}. % nlp in python 
    It is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms. 
    
    We made use of multiple tools from this library. 
    We used \texttt{Dictionary} object for vocabulary filtering and embedding into the BOW representation and we used its TF-IDF implementations.
    We also used its API to train word vectors. 
    
    Most important for us was gensims implementation of the probabilistic algorithm for constructing approximate matrix decomposition~\cite{halko2011finding}.
    This provides fast and easy to use incremental SVD decomposition.
    
    \subsubsection{Numpy}
    
    NumPy is a library for Python, adding support for large, multi-dimensional arrays and matrices~\cite{oliphant2006guide}. % numpy
    We used this library for result evaluation and to implement our own logistic regression.
    
    \subsubsection{TensorFlow}
    
    TensorFlow is an open source software library for high performance numerical computation developed by Google Brain team~\cite{tensorflow2015-whitepaper}. 
    It provides simple interface to matrix computations on GPUs and CPUs.
    
    One of the most important capabilities of this library is automatic differentiation.
    We can create symbolic expression representing the desired computation $\hat{y}=g(X\Theta)$ and this library can automatically compute the gradient $$\frac{\partial \hat{y}}{\Theta}$$
    Because of this, TensorFlow is very popular among  machine learning researchers.
    
    We wanted to implement our whole approach in TensorFlow.
    It even contains implementation of SVD, however that implementation does not have all the properties that we require.
    
    \subsection{Other tools}
    
    During the development of required code we worked in Jupyter Notebook. 
    It is an interactive environment the offers very easy prototyping and
    data visualization~\cite{PER-GRA:2007}. %ipython
    For data visualization we used the Matplotlib library~\cite{hunter2007matplotlib}. %matplotlib 
    Finally, for data manipulation and presentation we used the Pandas~\cite{mckinney2010data}. %pandas
    
    Last important mention belongs to Aysen Tatarinov, his blog and his repository \footnote{\url{https://github.com/aysent/supervised-term-weighting}}, where he implemented a number of supervised weight schemes~\cite{maas2011learning}.
    
    
    \subsubsection{Cloud} 
    Because we want to test multiple parameters on multiple datasets, the number of experiments we need to perform explodes.
    Running all those experiments on one PC would be unfeasible.
    
    We made use of the Google Cloud service \footnote{\url{https://cloud.google.com/}}.
    We choose Google cloud mainly because it is easy to use and it provides a free trial with $300\$$ of credit that can be spend on computation. 
    This credit equals to approximately $6$ moths of compute time on PC with $2$ CPUs and with $7.5$GB of memory (\texttt{n1-standard2}).
    
    Thanks to a good experiment design and a few bash scripts,
    we were able to run experiments on $10$ machines at parallel.
    
    
    In the end, our experiments costed approximately $40\$$ worth of computation time.
    