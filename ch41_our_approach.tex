\section{Our approach}
    
    In this section we present accuracy achieved by our approach.
    To address the problems of accuracy as a metric, we report relative improvements over the bias of given dataset.
    This can be viewed as an improvement over the precision of the constant classifier, but it is slightly more stable in this way. 
    For absolute accuracies refer to the appendix \ref{appendix:detailed}. 

    Note, that because we have multiple testing datasets and multiple methods, we witness a small combinatorial explosion.

    \subsection{Parameters}
    \* % describe those weights and add ref
    We test number of different weighting schemes: \texttt{tfidf}, \texttt{tfchi2}, \texttt{tfig}, \texttt{tfgr}, \texttt{tfor}, \texttt{tfrf}, \texttt{None};
    different embedding sizes: $200$, $300$, $400$;
    and different learning rates: $0.1$, $0.01$, $0.001$.
    We perform all of these tests on all of the $10$ datasets described in section \ref{sec:data:overview}.
    
    Together we need to perform and evaluate around $630$ experiments for one optimization routine.
    
    We compare our results against LSA baseline (LSA with given weights, but without training).
    Results for this baseline are in section \ref{sec:lsa:baseline}.
    
    \subsection{Batch gradient descent}
    We evaluate the performance of batch gradient descent optimization routine.
    We perform experiments for different weighting schemes, different sizes of embeddings and different learning rates on all datasets.

    Our results are in the table \ref{tab:batch:results} and table \ref{tab:batch:results:trec}.
    We present only results for $\alpha=0.1$, as this learning rate showed to be the best. \* % appendix other alphas
        
    \input{tables/batch_results.tex}

    Interesting observation is, that improvement consistently depends on the task, 
    but not much on the weighting scheme that was used or on the number of dimensions.
    We see consistent improvements on CR dataset and on TREC-DESC datasets.

    This means, that all those weighting schemes have some common shortcomings, 
    that are manifested only on some specific tasks.  \* % todo ref to the weight analysis part
    
    During the training w encountered 
    tfchi2 generated $M$ with less than 400 dimensions. 
    
    \subsubsection{Multiple gradient steps}
    
    We perform an experiment, where we do multiple gradient steps on $w'$ for each loop. 
    This optimization routine is illustrated by algorithm \ref{algo:batch:multiw}.
    
    \begin{algorithm}[H]
        \KwData{$M$}
        \KwResult{trained $W$, $S$, $C$ }
        $w'^0 = 1$, $t=1$\;
        \While{performance is improving on validation dataset, $t$++}{
            recompute $S^t$ from $M \circ w'^{t-1}$\;
            compute embeddings $v^t$ from $M \circ w'^{t-1}$ with $S^t$\;
            fully train $C^t$ on $v^t$ and labels $y$\;
            \For{$0\leq i < m$}{
                update $W^t$: $w'^t = w'^{t-1} - \alpha \frac{\partial E(C(S(M w'^{t-1}))}{\partial w'^{t-1}}$
            }
        }
        \caption{stochastic training of $w'$} \label{algo:batch:multiw}
    \end{algorithm}
    
    However this introduces another hyperparameter: number of performed $w'$ steps.
    We would like to to avoid new hyperparameters, so we do not spend too much resources on this experiment.
    Second problem with this approach is, that if we perform $m$ steps, the algorithm may be $m$ times slower. 
    This is a serious issue for $m>10$. 
    We perform experiments only for weighting scheme \texttt{None} with $200$ dimensional embedding and learning rate $0.1$.
    We perform $m=2$ and $m=5$ gradient steps and we present results in tables \ref{tab:multiw:notrec:2},
    \ref{tab:multiw:trec:2},\ref{tab:multiw:notrec:5},\ref{tab:multiw:trec:5}.
    
    \input{tables/batch_results_multiw}

    We conclude, that making multiple gradient steps on $w'$ does not significantly nor consistently improve the accuracy and only makes the training process slower.
    The intuition, that multiple gradient steps on $w'$ may decrease the convergence time showed to be wrong.




\begin{table}[H]
\begin{center}

\begin{tabular}{lrrrr}
\toprule
{} & CR & MPQA & MR & SUBJ \\
\$m\$ &      &       &      &       \\
\midrule
1  &     33 &      52 &     57 &      37 \\
2  &     32 &      43 &     45 &      46 \\
3  &     32 &      56 &     48 &      36 \\
4  &     33 &      68 &     63 &      39 \\
5  &     35 &      53 &     48 &      41 \\
\bottomrule
\end{tabular}

\caption[Number of needed training steps for different $w'$]{Number of needed training steps for different $w'$}
\label{tab:multyw:steps}
\end{center}
\end{table}


\begin{table}[H]
\begin{center}

\begin{tabular}{lrrrrrr}
\toprule
{} & ABBR & DESC & ENTY & HUM & LOC & NUM \\
\$m\$ &          &          &          &         &         &         \\
\midrule
1  &        32 &        38 &        38 &        72 &        38 &        48 \\
2  &        32 &        39 &        40 &        45 &        38 &        36 \\
3  &        32 &        48 &        35 &        38 &        32 &        38 \\
4  &        32 &        34 &        45 &        40 &        33 &        47 \\
5  &        32 &        37 &        36 &        32 &        37 &        52 \\
\bottomrule
\end{tabular}

\caption[Number of needed training steps for different $w'$ on TREC dataset]{Number of needed training steps for different $w'$ on TREC dataset}
\label{tab:multyw:steps:trec}
\end{center}
\end{table}

The only promising decrease in number of required learning steps can be seen for \texttt{TREC-HUM}
    
    \* %TODO, get learning times
    
    