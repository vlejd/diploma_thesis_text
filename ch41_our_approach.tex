\section{eLSA}
    
    In this section we present accuracy achieved by our approach.
    
    Note, that because we test on multiple datasets and we test multiple parameters, we witness a small combinatorial explosion.

    \subsection{Parameters}
    \* % describe those weights and add ref
    We test number of different weighting schemes: \texttt{tfidf}, \texttt{tfchi2}, \texttt{tfig}, \texttt{tfgr}, \texttt{tfor}, \texttt{tfrf}, \texttt{None};
    different embedding sizes: $200$, $300$, $400$;
    and different $w'$-learning rates $\beta$: $0.1$, $0.01$, $0.001$.
    We perform all of these tests on all of the $10$ datasets described in section \ref{sec:data:overview}.
    Together we need to perform and evaluate around $630$ experiments for one optimization routine.
    
    We compare our results against LSA baseline (LSA with given weights, but without training).
    Results for this baseline are in section \ref{sec:lsa:baseline}.
    
    \subsection{Batch gradient descent}
    We evaluate the performance of batch gradient descent optimization routine.
    We perform experiments for different weighting schemes, different sizes of embeddings and different learning rates on all datasets.

    Our results are in the table \ref{tab:batch:results} and table \ref{tab:batch:results:trec}.
    We present only results for $\beta=0.1$, as this learning rate showed to be the best. 
    
    \input{tables/batch_results.tex}

    In tables \ref{tab:batch:results:trec} and \ref{tab:batch:results} we see consistent improvements of eLSA over LSA.
    Moreover we see, that no scheme is actually locally optimal, and each scheme can be improved by performing the eLSA.
    There is only one dataset that we do not improve on and that is TREC-ABBR.
    However, this dataset is extremely hard and very biased. 
    
    These results mean, that all weighting schemes have some shortcomings, 
    that may be manifested on some specific tasks.  
    We try to explain this effect in section \ref{chap:weight:analysis}.
    
    During the training we observed an rather interesting behaviour for \texttt{tfchi2} weights on TREC dataset.
    When we computed the $400$ dimensional LSA embedding, it produced less than $398$ dimensions. 
    This means, that the original matrix was in fact just $398$ dimensional, 
    which means that the weighting scheme already filtered out a lot of information (hopefully noice) form the data. 

    \subsubsection{Multiple gradient steps}
    
    We perform an experiment, where we do multiple gradient steps on $w'$ for each loop. 
    This optimization routine is illustrated by algorithm \ref{algo:batch:multiw}.
    
    \begin{algorithm}[H]
        \KwData{$M$}
        \KwResult{trained $W$, $S$, $C$ }
        $w'^0 = 1$, $t=1$\;
        \While{performance is improving on validation dataset, $t$++}{
            recompute $S^t$ from $M \circ w'^{t-1}$\;
            compute embeddings $v^t$ from $M \circ w'^{t-1}$ with $S^t$\;
            fully train $C^t$ on $v^t$ and labels $y$\;
            \For{$0\leq i < m$}{
                update $W^t$: $w'^t = w'^{t-1} - \beta \frac{\partial E(C(S(M w'^{t-1}))}{\partial w'^{t-1}}$
            }
        }
        \caption{stochastic training of $w'$} \label{algo:batch:multiw}
    \end{algorithm}
    
    However this introduces another hyperparameter: number of performed $w'$ steps.
    We would like to to avoid new hyperparameters, so we do not spend too much resources on this experiment.
    Second problem with this approach is, that if we perform $m$ steps, the algorithm may be $m$ times slower. 
    This is a serious issue for $m>10$. 
    We perform experiments only for weighting scheme \texttt{None} with $200$ dimensional embedding and learning rate $0.1$.
    We perform $m=2$ and $m=5$ gradient steps and we present results in tables \ref{tab:multiw:notrec:2},
    \ref{tab:multiw:trec:2},\ref{tab:multiw:notrec:5},\ref{tab:multiw:trec:5}.
    
    \input{tables/batch_results_multiw}

    We conclude, that making multiple gradient steps on $w'$ does not significantly nor consistently improve the accuracy and only makes the training process slower.
    The intuition, that multiple gradient steps on $w'$ may decrease the convergence time also showed to be wrong.


\begin{table}[H]
\begin{center}

\begin{tabular}{lrrrr}
\toprule
{} & CR & MPQA & MR & SUBJ \\
\$m\$ &      &       &      &       \\
\midrule
1  &     33 &      52 &     57 &      37 \\
2  &     32 &      43 &     45 &      46 \\
3  &     32 &      56 &     48 &      36 \\
4  &     33 &      68 &     63 &      39 \\
5  &     35 &      53 &     48 &      41 \\
\bottomrule
\end{tabular}

\caption[Number of needed training steps for different $w'$]{Number of needed training steps for different $w'$}
\label{tab:multyw:steps}
\end{center}
\end{table}


\begin{table}[H]
\begin{center}

\begin{tabular}{lrrrrrr}
\toprule
{} & ABBR & DESC & ENTY & HUM & LOC & NUM \\
\$m\$ &          &          &          &         &         &         \\
\midrule
1  &        32 &        38 &        38 &        72 &        38 &        48 \\
2  &        32 &        39 &        40 &        45 &        38 &        36 \\
3  &        32 &        48 &        35 &        38 &        32 &        38 \\
4  &        32 &        34 &        45 &        40 &        33 &        47 \\
5  &        32 &        37 &        36 &        32 &        37 &        52 \\
\bottomrule
\end{tabular}

\caption[Number of needed training steps for different $w'$ on TREC dataset]{Number of needed training steps for different $w'$ on TREC dataset}
\label{tab:multyw:steps:trec}
\end{center}
\end{table}

    The only decrease in the number of required learning steps in tables \ref{tab:multyw:steps} and \ref{tab:multyw:steps:trec} can be seen for \texttt{TREC-HUM}.
    However this direction does not look very promising and we do not explore it further.
    